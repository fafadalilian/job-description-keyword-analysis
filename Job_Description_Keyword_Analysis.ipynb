{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "49fe09b1",
      "metadata": {
        "id": "49fe09b1"
      },
      "source": [
        "# Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49abd600",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-26T03:29:42.474593Z",
          "start_time": "2024-06-26T03:29:36.895619Z"
        },
        "id": "49abd600"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers torch scikit-learn nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3224e3cd",
      "metadata": {
        "id": "3224e3cd"
      },
      "source": [
        "# Import Libraries and Load BERT NER Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load pre-trained model and tokenizer for NER\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "# Create a pipeline for NER\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYhfKAnEWEGK",
        "outputId": "a18de901-fd4c-4a45-a43e-cfc7f8e93162"
      },
      "id": "PYhfKAnEWEGK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7557c3",
      "metadata": {
        "id": "2a7557c3"
      },
      "source": [
        "# Define Functions for Preprocessing and Keyword Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\W', ' ', text)   # Remove all non-word characters\n",
        "    text = re.sub(r'\\d', '', text)    # Remove all digits\n",
        "    return text.strip()\n",
        "\n",
        "def extract_ner_keywords(text):\n",
        "    ner_results = nlp(text)\n",
        "    keywords = [entity['word'] for entity in ner_results]\n",
        "    return keywords\n",
        "\n",
        "def extract_tfidf_keywords(text, top_n=10):\n",
        "    vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 3))\n",
        "    X = vectorizer.fit_transform([text])\n",
        "    indices = np.argsort(vectorizer.idf_)[::-1]\n",
        "    features = vectorizer.get_feature_names_out()\n",
        "    top_features = [features[i] for i in indices[:top_n]]\n",
        "    return top_features\n"
      ],
      "metadata": {
        "id": "YJnPxg9ZWJme"
      },
      "id": "YJnPxg9ZWJme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "91a8f187",
      "metadata": {
        "id": "91a8f187"
      },
      "source": [
        "# User Input for Job Descriptions and Keyword Extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_job_descriptions():\n",
        "    print(\"Enter job descriptions (type 'done' to finish):\")\n",
        "    job_descriptions = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if line.lower() == 'done':\n",
        "            break\n",
        "        job_descriptions.append(line)\n",
        "    return job_descriptions\n",
        "\n",
        "# Function to get and process job descriptions, then extract keywords\n",
        "def analyze_job_descriptions():\n",
        "    job_descriptions = get_job_descriptions()\n",
        "    all_keywords = []\n",
        "    for description in job_descriptions:\n",
        "        preprocessed_text = preprocess_text(description)\n",
        "        ner_keywords = extract_ner_keywords(description)\n",
        "        tfidf_keywords = extract_tfidf_keywords(preprocessed_text)\n",
        "        combined_keywords = list(set(ner_keywords + tfidf_keywords))  # Combine and remove duplicates\n",
        "        all_keywords.extend(combined_keywords)\n",
        "    unique_keywords = list(set(all_keywords))  # Remove duplicates from all keywords\n",
        "    print(\"\\nTop Keywords:\")\n",
        "    for keyword in unique_keywords:\n",
        "        print(keyword)\n",
        "\n",
        "# Run the analysis\n",
        "analyze_job_descriptions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWANIRu3THM-",
        "outputId": "369b6e25-9502-4299-8fb0-e70f387f7273"
      },
      "id": "kWANIRu3THM-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter job descriptions (type 'done' to finish):\n",
            "We are looking for a data scientist with experience in Python, machine learning, and data analysis. The ideal candidate will have expertise in Java, deep learning, and cloud computing\n",
            "done\n",
            "\n",
            "Top Keywords:\n",
            "experience python machine\n",
            "experience\n",
            "deep learning cloud\n",
            "Python\n",
            "experience python\n",
            "deep learning\n",
            "data scientist experience\n",
            "expertise\n",
            "data analysis ideal\n",
            "scientist experience python\n",
            "Java\n",
            "deep\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}